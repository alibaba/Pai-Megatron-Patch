includes:
        - base.yaml
        - model.yaml

load: null
# model partition
tensor_model_parallel_size: ${tensor_model_parallel_size:2}
pipeline_model_parallel_size: 1
num_inference_per_prompt: ${num_inference_per_prompt:8}
seq_length: ${seq_length:1024}
max_new_tokens: ${max_new_tokens:1023}

# sampling params
temperature: ${policy_temperature:1.0}
top_p: ${policy_top_p:0.9}
top_k: ${policy_top_k:-1}
presence_penalty: ${policy_presence_penalty:0.0}
frequency_penalty: ${policy_frequency_penalty:0.0}
repetition_penalty: ${policy_repetition_penalty:1.0}

eval_temperature: ${policy_eval_temperature:0.6}
eval_top_k: ${policy_eval_top_k:-1}
eval_top_p: ${policy_eval_top_p:0.9}
eval_presence_penalty: ${policy_eval_presence_penalty:0.0}
eval_frequency_penalty: ${policy_eval_frequency_penalty:0.0}
eval_repetition_penalty: ${policy_eval_repetition_penalty:1.0}

# dataset
vllm_prompt_key: ${vllm_prompt_key:prompt}
vllm_input_ids_key: ${vllm_input_ids_key:input_ids}

# scheduler config
max_num_batched_tokens: ${max_num_batched_tokens:32768}
max_seq_len_to_capture: ${max_seq_len_to_capture:32768}
enable_stage_resume: ${enable_policy_stage_resume:False}

# cache config
gpu_memory_utilization: ${gpu_memory_utilization:0.85}
enforce_eager: False

tokenizer: ${tokenizer_load}

